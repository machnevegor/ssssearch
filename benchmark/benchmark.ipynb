{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from os import getenv\n",
    "from typing import Optional\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from google.api_core import exceptions, retry\n",
    "from tqdm import tqdm\n",
    "from yarl import URL\n",
    "\n",
    "from src.scraper import Indexer, ScrapConfig, scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ScrapConfig(\n",
    "    root=URL(\"https://eduwiki.innopolis.university/index.php/Main_Page\"),\n",
    "    host=\"eduwiki.innopolis.university\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = await scrap(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry.Retry(predicate=retry.if_exception_type(exceptions.ResourceExhausted))\n",
    "def generate_answer(query: str, indexer: Indexer) -> str:\n",
    "    pages = indexer.search(query, 5)\n",
    "\n",
    "    if not pages:\n",
    "        return \"I'm sorry, I couldn't find any relevant information.\"\n",
    "\n",
    "    prompt = \"\"\"You are an Eduwiki intellectual assistant at Innopolis University. Your task is to analyze user queries based on the context provided. The context includes fragments of pages and their sources (URLs). You have to:\n",
    "\n",
    "1.\tRead the highlighted context between CONTEXT START and CONTEXT END.\n",
    "2.\tFind relevant information to answer the user's query using only the pages provided.\n",
    "3.\tProvide a concise and clear answer to the query.\n",
    "4.\tIf no relevant information is found in the context, respond: “I'm sorry, I couldn't find any relevant information.”\n",
    "5.\tCite sources using the format \"Source(s): \"\n",
    "\n",
    "CONTEXT START\"\"\"\n",
    "\n",
    "    for page in pages:\n",
    "        prompt += f\"\\n\\nSource: {page[0]}\\n\\n{page[1]}\"\n",
    "\n",
    "    prompt += f\"\\n\\nCONTEXT END\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "\n",
    "@retry.Retry(predicate=retry.if_exception_type(exceptions.ResourceExhausted))\n",
    "def judge_answer(\n",
    "    question: str, answer: str, source: str, generated: str\n",
    ") -> Optional[bool]:\n",
    "    prompt = f\"\"\"You are a judge model tasked with evaluating whether the generated answer is correct based on the provided data.\n",
    "\n",
    "Instructions:\n",
    "\t1.\tReview the following data:\n",
    "\t•\tQuestion: \"{question}\"\n",
    "\t•\tOriginal answer: \"{answer}\"\n",
    "\t•\tOriginal source: \"{source}\"\n",
    "\t•\tGenerated answer: \"{generated}\"\n",
    "\t2.\tEvaluate whether the generated answer:\n",
    "\t•\tMatches the accuracy and completeness of the original answer.\n",
    "\t•\tCorrectly cites the source.\n",
    "\n",
    "Please provide your evaluation as a boolean value (True/False).\"\"\"\n",
    "\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "\n",
    "    match response.text.strip():\n",
    "        case \"True\":\n",
    "            return True\n",
    "        case \"False\":\n",
    "            return False\n",
    "        case _:\n",
    "            print(question, answer, source, generated, response.text)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"qa.csv\")\n",
    "\n",
    "# Add a column for relevance\n",
    "df[\"Relevance\"] = None\n",
    "\n",
    "# Iterate over the dataset\n",
    "for row in tqdm(df.itertuples(), total=df.shape[0], desc=\"Benchmarking\"):\n",
    "    question, answer, source = row[1:4]\n",
    "\n",
    "    # Generate and judge answers\n",
    "    generated = generate_answer(question, indexer)\n",
    "    relevance = judge_answer(question, answer, source, generated)\n",
    "\n",
    "    # Save the relevance score\n",
    "    df.at[row.Index, \"Relevance\"] = relevance\n",
    "\n",
    "# Save the results\n",
    "df.to_csv(\"qa_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssssearch-423gvWoe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
